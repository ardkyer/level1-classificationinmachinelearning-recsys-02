{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 호출\n",
    "data_path: str = \"./data\"\n",
    "train_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"train.csv\")).assign(_type=\"train\") # train 에는 _type = train \n",
    "test_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"test.csv\")).assign(_type=\"test\") # test 에는 _type = test\n",
    "submission_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"test.csv\")) # ID, target 열만 가진 데이터 미리 호출\n",
    "df: pd.DataFrame = pd.concat([train_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:00<00:00, 122.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# HOURLY_ 로 시작하는 .csv 파일 이름을 file_names 에 할딩\n",
    "file_names: List[str] = [\n",
    "    f for f in os.listdir(data_path) if f.startswith(\"HOURLY_\") and f.endswith(\".csv\")\n",
    "]\n",
    "\n",
    "# 파일명 : 데이터프레임으로 딕셔너리 형태로 저장\n",
    "file_dict: Dict[str, pd.DataFrame] = {\n",
    "    f.replace(\".csv\", \"\"): pd.read_csv(os.path.join(data_path, f)) for f in file_names\n",
    "}\n",
    "\n",
    "for _file_name, _df in tqdm(file_dict.items()):\n",
    "    # 열 이름 중복 방지를 위해 {_file_name.lower()}_{col.lower()}로 변경, datetime 열을 ID로 변경\n",
    "    _rename_rule = {\n",
    "        col: f\"{_file_name.lower()}_{col.lower()}\" if col != \"datetime\" else \"ID\"\n",
    "        for col in _df.columns\n",
    "    }\n",
    "    _df = _df.rename(_rename_rule, axis=1)\n",
    "    df = df.merge(_df, on=\"ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "seed=42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11552, 19)"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델에 사용할 컬럼, 컬럼의 rename rule을 미리 할당함\n",
    "cols_dict: Dict[str, str] = {\n",
    "    \"ID\": \"ID\",\n",
    "    \"target\": \"target\",\n",
    "    \"_type\": \"_type\",\n",
    "    \"hourly_market-data_coinbase-premium-index_coinbase_premium_gap\": \"coinbase_premium_gap\",\n",
    "    \"hourly_market-data_coinbase-premium-index_coinbase_premium_index\": \"coinbase_premium_index\",\n",
    "    \"hourly_market-data_funding-rates_all_exchange_funding_rates\": \"funding_rates\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_long_liquidations\": \"long_liquidations\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_long_liquidations_usd\": \"long_liquidations_usd\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_short_liquidations\": \"short_liquidations\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_short_liquidations_usd\": \"short_liquidations_usd\",\n",
    "    \"hourly_market-data_open-interest_all_exchange_all_symbol_open_interest\": \"open_interest\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_ratio\": \"buy_ratio\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_sell_ratio\": \"buy_sell_ratio\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_volume\": \"buy_volume\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_sell_ratio\": \"sell_ratio\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_sell_volume\": \"sell_volume\",\n",
    "    \"hourly_network-data_addresses-count_addresses_count_active\": \"active_count\",\n",
    "    \"hourly_network-data_addresses-count_addresses_count_receiver\": \"receiver_count\",\n",
    "    \"hourly_network-data_addresses-count_addresses_count_sender\": \"sender_count\",\n",
    "}\n",
    "df = df[cols_dict.keys()].rename(cols_dict, axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda 에서 파악한 차이와 차이의 음수, 양수 여부를 새로운 피쳐로 생성\n",
    "df = df.assign(\n",
    "    liquidation_diff=df[\"long_liquidations\"] - df[\"short_liquidations\"],\n",
    "    liquidation_usd_diff=df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"],\n",
    "    volume_diff=df[\"buy_volume\"] - df[\"sell_volume\"],\n",
    "    liquidation_diffg=np.sign(df[\"long_liquidations\"] - df[\"short_liquidations\"]),\n",
    "    liquidation_usd_diffg=np.sign(df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"]),\n",
    "    volume_diffg=np.sign(df[\"buy_volume\"] - df[\"sell_volume\"]),\n",
    "    buy_sell_volume_ratio=df[\"buy_volume\"] / (df[\"sell_volume\"] + 1),\n",
    ")\n",
    "# category, continuous 열을 따로 할당해둠\n",
    "category_cols: List[str] = [\"liquidation_diffg\", \"liquidation_usd_diffg\", \"volume_diffg\"]\n",
    "conti_cols: List[str] = [_ for _ in cols_dict.values() if _ not in [\"ID\", \"target\", \"_type\"]] + [\n",
    "    \"buy_sell_volume_ratio\",\n",
    "    \"liquidation_diff\",\n",
    "    \"liquidation_usd_diff\",\n",
    "    \"volume_diff\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sub_target\"] = df[\"target\"].apply(lambda x: 1 if x >= 2 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_feature(\n",
    "    df: pd.DataFrame,\n",
    "    conti_cols: List[str],\n",
    "    intervals: List[int],\n",
    ") -> List[pd.Series]:\n",
    "    \"\"\"\n",
    "    연속형 변수의 shift feature 생성\n",
    "    Args:\n",
    "        df (pd.DataFrame)\n",
    "        conti_cols (List[str]): continuous colnames\n",
    "        intervals (List[int]): shifted intervals\n",
    "    Return:\n",
    "        List[pd.Series]\n",
    "    \"\"\"\n",
    "    df_shift_dict = [\n",
    "        df[conti_col].shift(interval).rename(f\"{conti_col}_{interval}\")\n",
    "        for conti_col in conti_cols\n",
    "        for interval in intervals\n",
    "    ]\n",
    "    # future shift df\n",
    "    # df_future_shift_dict = [\n",
    "    #     df[conti_col].shift(-interval).rename(f\"{conti_col}_{-interval}\")\n",
    "    #     for conti_col in conti_cols\n",
    "    #     for interval in intervals\n",
    "    # ]\n",
    "    # df_shift_dict.extend(df_future_shift_dict)\n",
    "    \n",
    "    return df_shift_dict\n",
    "\n",
    "# test에서 shift 된 것은 사용할 수 없기 때문에 test에서 shift하기 전에 train, test를 분리\n",
    "train_df: pd.DataFrame = df.loc[df[\"_type\"] == \"train\"]\n",
    "test_df: pd.DataFrame = df.loc[df[\"_type\"] == \"test\"]\n",
    "train_df = train_df.drop(\"_type\", axis=1)\n",
    "test_df = test_df.drop(\"_type\", axis=1)\n",
    "\n",
    "train_shift_list = shift_feature(\n",
    "    df=train_df, conti_cols=conti_cols, intervals=[_ for _ in range(1, 24)]\n",
    ")\n",
    "test_shift_list = shift_feature(\n",
    "    df=test_df, conti_cols=conti_cols, intervals=[_ for _ in range(1, 24)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat 하여 df 에 할당\n",
    "train_df = pd.concat([train_df, pd.concat(train_shift_list, axis=1)], axis=1)\n",
    "test_df = pd.concat([test_df, pd.concat(test_shift_list, axis=1)], axis=1)\n",
    "\n",
    "_target = train_df[\"target\"]\n",
    "train_df = train_df.ffill().fillna(-999).assign(target = _target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/venv/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04772351 0.36522757 0.52773973 0.05930919]\n",
      " [0.07512526 0.46812406 0.40794568 0.048805  ]\n",
      " [0.06298173 0.42031876 0.44661894 0.07008058]\n",
      " ...\n",
      " [0.09574566 0.36012216 0.44876471 0.09536747]\n",
      " [0.06425714 0.42336402 0.37548182 0.13689702]\n",
      " [0.04171443 0.51829904 0.35301667 0.08696986]]\n",
      "acc: 0.4280821917808219, auroc: 0.5987788886723954\n"
     ]
    }
   ],
   "source": [
    "# train_test_split 으로 valid set, train set 분리\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    train_df.drop([\"target\", \"ID\", \"sub_target\"], axis = 1), \n",
    "    train_df[\"target\"].astype(int), \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"target\"],\n",
    ")\n",
    "\n",
    "# lgb dataset\n",
    "train_data = lgb.Dataset(x_train, label=y_train)\n",
    "valid_data = lgb.Dataset(x_valid, label=y_valid, reference=train_data)\n",
    "\n",
    "# lgb params\n",
    "# params = {\n",
    "#     \"boosting_type\": \"gbdt\",\n",
    "#     \"objective\": \"multiclass\",\n",
    "#     \"metric\": \"multi_logloss\",\n",
    "#     \"num_class\": 4,\n",
    "#     \"num_leaves\": 12,\n",
    "#     \"learning_rate\": 0.05,\n",
    "#     \"n_estimators\": 7,\n",
    "#     \"random_state\": 42,\n",
    "#     \"verbose\": 0,\n",
    "# }\n",
    "params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"metric\": \"multi_logloss\",\n",
    "    \"num_class\": 4,\n",
    "    \"num_leaves\": 50,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 30,\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": 0,\n",
    "}\n",
    "\n",
    "# lgb train\n",
    "lgb_model = lgb.train(\n",
    "    params=params,\n",
    "    train_set=train_data,\n",
    "    valid_sets=valid_data,\n",
    ")\n",
    "\n",
    "# lgb predict\n",
    "y_valid_pred = lgb_model.predict(x_valid)\n",
    "print(y_valid_pred)\n",
    "y_valid_pred_class = np.argmax(y_valid_pred, axis = 1)\n",
    "\n",
    "# score check\n",
    "accuracy = accuracy_score(y_valid, y_valid_pred_class)\n",
    "auroc = roc_auc_score(y_valid, y_valid_pred, multi_class=\"ovr\")\n",
    "\n",
    "print(f\"acc: {accuracy}, auroc: {auroc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "2    1810\n",
       "1     948\n",
       "3      29\n",
       "0       5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test predict\n",
    "test_df = test_df.drop([\"target\", \"ID\", \"sub_target\"], axis = 1)\n",
    "test_pred = lgb_model.predict(test_df)\n",
    "test_pred_class = np.argmax(test_pred, axis = 1)\n",
    "submission_df[\"target\"] = test_pred_class\n",
    "submission_df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/venv/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.53998006 0.53998006 0.46001994 0.46001994]\n",
      " [0.45564541 0.45564541 0.54435459 0.54435459]\n",
      " [0.50229933 0.50229933 0.49770067 0.49770067]\n",
      " ...\n",
      " [0.45773468 0.45773468 0.54226532 0.54226532]\n",
      " [0.54714097 0.54714097 0.45285903 0.45285903]\n",
      " [0.47944265 0.47944265 0.52055735 0.52055735]]\n",
      "0.5188356164383562\n"
     ]
    }
   ],
   "source": [
    "# train_test_split 으로 valid set, train set 분리\n",
    "x_train_split, x_valid_split, y_train_split, y_valid_split = train_test_split(\n",
    "    train_df.drop([\"target\", \"ID\", \"sub_target\"], axis = 1), \n",
    "    train_df[\"sub_target\"].astype(int), \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"sub_target\"],\n",
    ")\n",
    "\n",
    "# lgb dataset\n",
    "train_data_split = lgb.Dataset(x_train_split, label=y_train_split)\n",
    "valid_data_split = lgb.Dataset(x_valid_split, label=y_valid_split, reference=train_data_split)\n",
    "\n",
    "# lgb params\n",
    "# params_split = {\n",
    "#     \"boosting_type\": \"gbdt\",\n",
    "#     # \"objective\": \"multiclass\",\n",
    "#     'objective': 'binary',\n",
    "#     \"metric\": \"binary_logloss\",\n",
    "#     # \"metric\": \"multi_logloss\",\n",
    "#     # \"num_class\": 4,\n",
    "#     \"num_leaves\": 12,\n",
    "#     \"learning_rate\": 0.05,\n",
    "#     \"n_estimators\": 7,\n",
    "#     \"random_state\": 42,\n",
    "#     \"verbose\": 0,\n",
    "# }\n",
    "params_split = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    'objective': 'binary',\n",
    "   \"metric\": \"binary_logloss\",\n",
    "    \"num_leaves\": 50,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 30,\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": 0,\n",
    "}\n",
    "\n",
    "# lgb train\n",
    "lgb_model_split = lgb.train(\n",
    "    params=params_split,\n",
    "    train_set=train_data_split,\n",
    "    valid_sets=valid_data_split,\n",
    ")\n",
    "\n",
    "# lgb predict\n",
    "y_valid_pred_split = lgb_model_split.predict(x_valid_split)\n",
    "# y_valid_pred_class = np.argmax(y_valid_pred, axis = 1)\n",
    "# print(y_valid_pred[:20])\n",
    "y_valid_pred_4 = np.array([[1-p, 1-p, p, p] for p in y_valid_pred_split])\n",
    "print(y_valid_pred_4)\n",
    "y_valid_pred_class_split = np.where(y_valid_pred_split > 0.5, 1, 0)\n",
    "# score check\n",
    "accuracy_split = accuracy_score(y_valid_split, y_valid_pred_class_split)\n",
    "# auroc = roc_auc_score(y_valid, y_valid_pred, multi_class=\"ovr\")\n",
    "\n",
    "# print(f\"acc: {accuracy}, auroc: {auroc}\")\n",
    "print(accuracy_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4394977168949772 0.43994399439944\n"
     ]
    }
   ],
   "source": [
    "# lgb, lgb_split ensemble coef search\n",
    "best_acc = 0\n",
    "best_coef = 0\n",
    "for coef in np.linspace(0, 1, 10000):\n",
    "    y_valid_pred_ensemble = y_valid_pred * coef + y_valid_pred_4 * (1 - coef)\n",
    "    y_valid_pred_class = np.argmax(y_valid_pred_ensemble, axis = 1)\n",
    "    accuracy = accuracy_score(y_valid, y_valid_pred_class)\n",
    "    if accuracy > best_acc:\n",
    "        best_acc = accuracy\n",
    "        best_coef = coef\n",
    "print(best_acc, best_coef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test predict and submission\n",
    "# test_df = test_df.drop([\"target\", \"ID\", \"sub_target\"], axis = 1)\n",
    "test_pred = lgb_model.predict(test_df)\n",
    "test_pred_split = lgb_model_split.predict(test_df)\n",
    "test_pred_4 = np.array([[1-p, 1-p, p, p] for p in test_pred_split])\n",
    "test_pred_ensemble = test_pred * best_coef + test_pred_4 * (1 - best_coef)\n",
    "test_pred_class = np.argmax(test_pred_ensemble, axis = 1)\n",
    "submission_df[\"target\"] = test_pred_class\n",
    "submission_df.to_csv(\"./split_model_ensemble.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "2    1670\n",
       "1    1100\n",
       "3      17\n",
       "0       5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df[\"target\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
